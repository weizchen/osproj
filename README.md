# GROSR: GPU-Resident Operating System Runtime

**GROSR** eliminates the CPU control-plane bottleneck for fine-grained, dynamic GPU workloads by moving scheduling and memory allocation onto the GPU itself.

## Project Structure

```
osproj/
â”œâ”€â”€ Core Runtime (Required)
â”‚   â”œâ”€â”€ grosr_runtime.h          # Unified GROSR runtime header
â”‚   â”œâ”€â”€ grosr_allocator.cu       # GPU-side slab allocator
â”‚   â”œâ”€â”€ grosr_queue.cu           # Task queue management
â”‚   â””â”€â”€ grosr_runtime.cu         # Persistent runtime kernel
â”‚
â”œâ”€â”€ Main Experiments (Current)
â”‚   â”œâ”€â”€ exp_a_pingpong.cu        # Experiment A: Ping-Pong Latency
â”‚   â”œâ”€â”€ exp_b_throughput.cu      # Experiment B: Throughput Benchmark
â”‚   â”œâ”€â”€ exp_c_graph_bfs.cu      # Experiment C: Graph BFS Macrobenchmark
â”‚   â””â”€â”€ test_allocator.cu        # Allocator unit tests
â”‚
â”œâ”€â”€ Legacy Demos (Reference Only)
â”‚   â”œâ”€â”€ benchmark_exp0.cu        # Original benchmark (superseded by exp_a/b)
â”‚   â”œâ”€â”€ exp0_microkernel.cu       # Original microkernel demo (superseded by unified runtime)
â”‚   â””â”€â”€ exp1_syscalls.cu         # GPU syscall proxy demo (proof of concept)
â”‚
â”œâ”€â”€ Build & Scripts
â”‚   â”œâ”€â”€ Makefile                 # Build system
â”‚   â””â”€â”€ run_experiments.py       # Legacy Python script (for benchmark_exp0.cu)
â”‚
â””â”€â”€ Documentation
    â”œâ”€â”€ README.md                # This file
    â”œâ”€â”€ RESEARCH_PLAN.md         # Detailed research plan
    â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md # Implementation details
    â”œâ”€â”€ GEMINI_CHAT.md          # Historical reference (chat logs)
    â””â”€â”€ GPT_CHAT.md             # Historical reference (chat logs)
```

## Building

### Prerequisites
- NVIDIA GPU with CUDA support (sm_70+)
- CUDA Toolkit 11.0+
- Python 3.x with matplotlib (for plotting)

### Build Commands

```bash
# Build main experiments (recommended)
make main

# Build all executables (including legacy demos)
make all

# Build for specific architecture (e.g., A100)
make ARCH=sm_80 main

# Build specific experiment
make exp_a_pingpong
make exp_b_throughput
make exp_c_graph_bfs

# Run tests
make test

# Run benchmarks
make benchmark

# Clean build artifacts
make clean
```

## Main Experiments

### Experiment A: Ping-Pong Latency
Measures the time to launch execution of a dependent task.

**Baseline**: CPU launches Kernel 1 â†’ CPU reads â†’ CPU launches Kernel 2  
**GROSR**: GPU Kernel 1 â†’ GPU Scheduler â†’ GPU Kernel 2 (no CPU)

```bash
./exp_a_pingpong [num_tasks]
```

**Expected Result**: 5-20Ã— reduction in latency (0.5Î¼s vs 10Î¼s)

### Experiment B: Throughput Benchmark
Measures throughput with many small tasks, includes statistical analysis.

**Baseline**: Standard CUDA kernel launches in loop  
**GROSR**: Persistent kernel with task queue

```bash
./exp_b_throughput [num_tasks] [iterations]
```

**Expected Result**: 10-50Ã— improvement in throughput

### Experiment C: Graph BFS Macrobenchmark
Demonstrates real-world benefit in dynamic, irregular workload (graph traversal).

**Baseline**: CPU checks queue size â†’ launches kernel â†’ GPU processes â†’ CPU checks again  
**GROSR**: GPU threads add neighbors to queue dynamically â†’ GPU scheduler processes autonomously

```bash
./exp_c_graph_bfs [num_nodes] [edge_probability] [source_node]
```

**Expected Result**: 1.2-2Ã— end-to-end speedup with reduced CPU overhead

### Allocator Test
Tests GPU-side slab allocator functionality.

```bash
./test_allocator
```

### Experiment D: Allocator Microbenchmark
Compares GROSR `gpu_malloc/gpu_free` against CUDA **device-side** `malloc/free`.

```bash
./exp_d_allocator_bench [num_threads] [iters_per_thread] [size_bytes] [mode] [outstanding] [touch_bytes]
```

## Key Components

### GPU-Side Slab Allocator
- **Location**: `grosr_allocator.cu`
- **Functions**: `gpu_malloc()`, `gpu_free()`
- **Size Classes**: 32B, 64B, 128B, 256B, 512B, 1KB, 2KB, 4KB
- **Purpose**: Enable dynamic memory allocation entirely on GPU

### Persistent Runtime Kernel
- **Location**: `grosr_runtime.cu`
- **Function**: `grosr_runtime_kernel()`
- **Purpose**: Scheduler loop that processes tasks from queue without CPU intervention

### Task Queue
- **Location**: `grosr_queue.cu`
- **Functions**: `init_task_queue()`, `push_task()`, `pop_task()`
- **Purpose**: CPU-GPU IPC via Unified Memory ring buffer

## Usage Example

```cpp
#include "grosr_runtime.h"

// Initialize
TaskQueue q;
init_task_queue(&q, 1024, sizeof(SimpleTask));

volatile int* stop_flag;
cudaMallocManaged((int**)&stop_flag, sizeof(int));
*stop_flag = 0;

GROSRRuntime runtime;
runtime.task_queue = q;
runtime.stop_flag = stop_flag;
runtime.results = d_results;

// Launch persistent runtime
grosr_runtime_kernel<<<1, 1>>>(runtime);

// Push tasks from CPU
SimpleTask task;
task.task_id = 0;
task.data = 42;
push_task(&q, task);

// Cleanup
*stop_flag = 1;
cudaDeviceSynchronize();
cleanup_task_queue(&q);
```

## Research Plan

See `RESEARCH_PLAN.md` for:
- Detailed research objectives
- Experimental design
- Timeline and milestones
- Related work analysis

## Status

### Phase 1: Foundation âœ… COMPLETED
- [x] Unified GROSR runtime header
- [x] Persistent GPU microkernel
- [x] CPU-GPU communication mechanisms
- [x] Basic benchmarking infrastructure

### Phase 2: Core Components ðŸ”„ IN PROGRESS
- [x] GPU-side slab allocator
- [x] Experiment A (ping-pong latency)
- [x] Experiment B (throughput with statistics)
- [~] Experiment C (macrobenchmark - Graph BFS prototype; needs validation/tuning)

## Writing the Report / Reproducing Results

The main report lives in `PROJECT.md`. To collect numbers for tables/plots:

```bash
make clean
make ARCH=sm_70 main    # or sm_80 for A100, sm_90 for H100
make test
./exp_a_pingpong 1000
./exp_b_throughput 10000 10
./exp_c_graph_bfs 10000 0.01 0
```

Each experiment prints CSV-style output that can be pasted into your report tables/plots.

## Contributing

This is a research project. For questions or issues, refer to the research plan or contact the project maintainer.

## License

Research project - see course guidelines.

